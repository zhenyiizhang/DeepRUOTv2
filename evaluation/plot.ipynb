{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import torch\n",
    "import anndata as ad\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from DeepRUOT.losses import OT_loss1\n",
    "from DeepRUOT.utils import (\n",
    "    generate_steps, load_and_merge_config,\n",
    "    SchrodingerBridgeConditionalFlowMatcher,\n",
    "    generate_state_trajectory, get_batch, get_batch_size\n",
    ")\n",
    "from DeepRUOT.train import train_un1_reduce, train_all\n",
    "from DeepRUOT.models import FNet, scoreNet2\n",
    "from DeepRUOT.constants import DATA_DIR, RES_DIR\n",
    "from DeepRUOT.exp import setup_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = '../config/weinreb_config.yaml'\n",
    "\n",
    "# Load and merge configuration\n",
    "config = load_and_merge_config(config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_DIR, config['data']['file_path']))\n",
    "df = df.iloc[:, :config['data']['dim'] + 1]\n",
    "device = torch.device('cpu')\n",
    "exp_dir, logger = setup_exp(\n",
    "            RES_DIR, \n",
    "            config, \n",
    "            config['exp']['name']\n",
    "        )\n",
    "dim = config['data']['dim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = config['model']\n",
    "        \n",
    "f_net = FNet(\n",
    "    in_out_dim=model_config['in_out_dim'],\n",
    "    hidden_dim=model_config['hidden_dim'],\n",
    "    n_hiddens=model_config['n_hiddens'],\n",
    "    activation=model_config['activation']\n",
    ").to(device)\n",
    "\n",
    "sf2m_score_model = scoreNet2(\n",
    "    in_out_dim=model_config['in_out_dim'],\n",
    "    hidden_dim=model_config['score_hidden_dim'],\n",
    "    activation=model_config['activation']\n",
    ").float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_net.load_state_dict(torch.load(os.path.join(exp_dir, 'model_final'),map_location=torch.device('cpu')))\n",
    "f_net.to(device)\n",
    "sf2m_score_model.load_state_dict(torch.load(os.path.join(exp_dir, 'score_model_final'),map_location=torch.device('cpu')))\n",
    "sf2m_score_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dim reduction (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.decomposition import PCA  # Import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import umap\n",
    "umap_op = umap.UMAP(n_components=2, random_state=42) # You may change UMAP to PCA or other dimension reduction methods\n",
    "xu = umap_op.fit_transform(df.iloc[:, 1:])  # Assuming df is your DataFrame\n",
    "joblib.dump(umap_op, os.path.join(exp_dir, 'dim_reduction.pkl'))  # Save the UMAP model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import joblib\n",
    "\n",
    "# Load dimension reducer\n",
    "dim_reducer = joblib.load(os.path.join(exp_dir, 'dim_reduction.pkl')) # If no dim reducer is needed, set this to None\n",
    "device = 'cpu'\n",
    "f_net = f_net.to(device)\n",
    "\n",
    "def plot_g_values(df, f_net, dim_reducer=None, device=device):\n",
    "    # Get all time points\n",
    "    time_points = df['samples'].unique()\n",
    "    \n",
    "    # Store data for each time point\n",
    "    data_by_time = {}\n",
    "    \n",
    "    # Calculate g_values for each time point\n",
    "    for time in time_points:\n",
    "        subset = df[df['samples'] == time]\n",
    "        n = dim  # Make sure dim is defined\n",
    "\n",
    "        # Generate column names\n",
    "        column_names = [f'x{i}' for i in range(1, n + 1)]\n",
    "\n",
    "        # Convert each column to tensor and move to device\n",
    "        tensors = [torch.tensor(subset[col].values, dtype=torch.float32).to(device) for col in column_names]\n",
    "\n",
    "        # Stack tensors into 2D tensor\n",
    "        data = torch.stack(tensors, dim=1)\n",
    "        with torch.no_grad():\n",
    "            t = torch.tensor([time], dtype=torch.float32).to(device)\n",
    "            _, g, _, _ = f_net(t, data)\n",
    "        \n",
    "        data_by_time[time] = {'data': subset, 'g_values': g.detach().cpu().numpy()}\n",
    "    \n",
    "    # Combine all g_values\n",
    "    all_g_values = np.concatenate([content['g_values'] for content in data_by_time.values()])\n",
    "    \n",
    "    # Calculate 95th percentile of g_values\n",
    "    vmax_value = np.percentile(all_g_values, 95)\n",
    "    \n",
    "    # Initialize color mapper with clipping\n",
    "    norm = plt.Normalize(vmin=0, vmax=vmax_value, clip=True)\n",
    "    \n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Plot data for each time point on same axis\n",
    "    for time, content in data_by_time.items():\n",
    "        subset = content['data']\n",
    "        g_values = content['g_values']\n",
    "        n = dim\n",
    "\n",
    "        column_names = [f'x{i}' for i in range(1, n + 1)]\n",
    "        new_data = subset[column_names]\n",
    "        \n",
    "        if dim_reducer is not None:\n",
    "            data_reduced = dim_reducer.transform(new_data)\n",
    "        else:\n",
    "            data_reduced = new_data.iloc[:, :2].values\n",
    "            \n",
    "        x = data_reduced[:, 0]\n",
    "        y = data_reduced[:, 1]\n",
    "        \n",
    "        # Map g_values to colors\n",
    "        colors = plt.cm.rainbow(norm(g_values))\n",
    "        \n",
    "        # Plot scatter with labels for legend\n",
    "        ax.scatter(x, y, c=colors, label=f'Time {time}', alpha=0.7, marker='o')\n",
    "    \n",
    "    ax.set_xlabel('Gene $X_1$')\n",
    "    ax.set_ylabel('Gene $X_2$')\n",
    "    \n",
    "    ax.legend()\n",
    "    \n",
    "    # Add colorbar\n",
    "    sm = plt.cm.ScalarMappable(cmap='rainbow', norm=norm)\n",
    "    sm.set_array(all_g_values)\n",
    "    cbar = fig.colorbar(sm, ax=ax)\n",
    "    cbar.set_label('Normalized predicted growth rate')\n",
    "    \n",
    "    # Format colorbar ticks\n",
    "    cbar.ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{(x):.2f}'))\n",
    "    \n",
    "    # Save as PDF\n",
    "    plt.savefig(os.path.join(exp_dir, 'g_values_plot.pdf'), format='pdf', bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Plot with f_net and df\n",
    "plot_g_values(df, f_net, dim_reducer=dim_reducer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "# Assume the following are defined:\n",
    "# - df: DataFrame with columns 'samples', 'x1', 'x2', ..., 'xn'\n",
    "# - dim: Number of dimensions (n)\n",
    "# - device: torch.device (e.g., 'cpu' or 'cpu')\n",
    "# - sf2m_score_model: Your model function that takes t_tensor and data_tensor\n",
    "\n",
    "# Step 1: Extract all time points and data points\n",
    "\n",
    "\n",
    "all_times = df['samples'].values\n",
    "all_data = df[[f'x{i}' for i in range(1, dim + 1)]].values\n",
    "\n",
    "#t_value = 2.0  \n",
    "t_tensor = torch.tensor(all_times).unsqueeze(1).float().to(device)\n",
    "\n",
    "# Step 2: Convert to PyTorch tensors\n",
    "data_tensor = torch.tensor(all_data, dtype=torch.float32).to(device)\n",
    "data_tensor.requires_grad_(True)  # Enable gradient tracking\n",
    "#t_tensor = torch.tensor(all_times, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "lnw0 = torch.log(torch.ones(data_tensor.shape[0], 1) / data_tensor.shape[0]).to(device)\n",
    "# Step 3: Compute log density values and gradients\n",
    "gradients = f_net.v_net(t_tensor, data_tensor) \n",
    "\n",
    "# Step 4: Move data to CPU for plotting\n",
    "data_np = data_tensor.detach().cpu().numpy()\n",
    "if dim_reducer is not None:\n",
    "    data_np_2d=dim_reducer.transform(data_np)\n",
    "else:\n",
    "    data_np_2d=data_np[:,:2]\n",
    "gradients_np = gradients.detach().cpu().numpy()\n",
    "data_end = data_np + gradients_np\n",
    "if dim_reducer is not None:\n",
    "    data_end_2d=dim_reducer.transform(data_end)\n",
    "else:\n",
    "    data_end_2d=data_end[:,:2]\n",
    "gradients_np=data_end_2d - data_np_2d\n",
    "\n",
    "gradients_np = gradients_np / np.linalg.norm(gradients_np, axis = 1, keepdims=True) * 5\n",
    "times_np = all_times\n",
    "data_np = data_np_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata\n",
    "import scvelo as scv\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "\n",
    "# Assume your data is already defined\n",
    "# all_data: 50-dimensional data with shape (n_cells, 50)\n",
    "# gradients: 50-dimensional vector field with shape (n_cells, 50) \n",
    "# pca: Trained PCA or UMAP model for generating X_umap\n",
    "# X_umap: Pre-computed UMAP embeddings with shape (n_cells, 2)\n",
    "\n",
    "# Create AnnData object\n",
    "adata = anndata.AnnData(X=all_data)\n",
    "\n",
    "# Set 'Ms' layer to avoid KeyError: 'Ms'\n",
    "adata.layers['Ms'] = all_data  # Use original 50-dim data as state matrix\n",
    "# Optional: normalize and log transform (based on data needs)\n",
    "# sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "# sc.pp.log1p(adata)\n",
    "# adata.layers['Ms'] = adata.X.copy()\n",
    "\n",
    "# Set velocity vectors\n",
    "adata.layers['velocity'] = gradients.detach().cpu().numpy()  # Store velocity vectors in layers\n",
    "print(adata.layers['velocity'].shape)  # Confirm shape\n",
    "\n",
    "# Set pre-computed UMAP embeddings\n",
    "if dim_reducer is not None:\n",
    "    X_umap = dim_reducer.transform(all_data)  # Assume pca is trained dim reduction model\n",
    "else:\n",
    "    X_umap = all_data[:,:2]\n",
    "adata.obsm['X_umap'] = X_umap\n",
    "print(adata.obsm['X_umap'].shape)  # Confirm UMAP embedding shape\n",
    "adata.obs['time'] = all_times\n",
    "if adata.layers['velocity'].shape[1] != 2:\n",
    "    # Calculate neighbor graph (required for velocity graph)\n",
    "    sc.pp.neighbors(adata, n_neighbors=30, use_rep='X')  # Calculate neighbors based on high-dim data\n",
    "\n",
    "    # Calculate velocity graph\n",
    "    scv.tl.velocity_graph(adata, vkey='velocity', n_jobs=16)  # Build velocity graph from high-dim velocity vectors\n",
    "\n",
    "    # Project velocities to UMAP space\n",
    "    scv.tl.velocity_embedding(adata, basis='umap', vkey='velocity')  # Project velocities to UMAP\n",
    "else:\n",
    "    adata.obsm['velocity_umap'] = adata.layers['velocity']\n",
    "\n",
    "# Plot\n",
    "\n",
    "adata.obs['time_categorical'] = pd.Categorical(adata.obs['time'])\n",
    "\n",
    "# Visualization settings\n",
    "scv.settings.set_figure_params('scvelo')  # Set scvelo plotting style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scv.pl.velocity_embedding_stream(\n",
    "    adata,\n",
    "    basis='umap',\n",
    "    color='time_categorical',\n",
    "    figsize=(7, 5),\n",
    "    density=3,\n",
    "    title='Velocity Stream Plot',\n",
    "    legend_loc='right',\n",
    "    palette='plasma',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Assume the following are defined:\n",
    "# - df: DataFrame with columns 'samples', 'x1', 'x2', ..., 'xn'\n",
    "# - dim: Number of dimensions (n)\n",
    "# - device: torch.device (e.g., 'cpu' or 'cpu')\n",
    "# - sf2m_score_model: Your model function that takes t_tensor and data_tensor\n",
    "\n",
    "# Step 1: Extract all time points and data points\n",
    "\n",
    "\n",
    "all_times = df['samples'].values\n",
    "all_data = df[[f'x{i}' for i in range(1, dim + 1)]].values\n",
    "\n",
    "# Step 2: Convert to PyTorch tensors\n",
    "data_tensor = torch.tensor(all_data, dtype=torch.float32).to(device)\n",
    "data_tensor.requires_grad_(True)  # Enable gradient tracking\n",
    "t_tensor = torch.tensor(all_times, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "# Step 3: Compute log density values and gradients\n",
    "log_density_values = sf2m_score_model(t_tensor, data_tensor)\n",
    "log_density_values.backward(torch.ones_like(log_density_values))\n",
    "gradients = data_tensor.grad\n",
    "\n",
    "# Step 4: Move data to CPU for plotting\n",
    "data_np = data_tensor.detach().cpu().numpy()\n",
    "if dim_reducer is not None:\n",
    "    data_np_2d=dim_reducer.transform(data_np)\n",
    "else:\n",
    "    data_np_2d=data_np[:,:2]\n",
    "gradients_np = gradients.detach().cpu().numpy()\n",
    "data_end = data_np + gradients_np\n",
    "if dim_reducer is not None:\n",
    "    data_end_2d=dim_reducer.transform(data_end)\n",
    "else:\n",
    "    data_end_2d=data_end[:,:2]\n",
    "gradients_np=data_end_2d - data_np_2d\n",
    "\n",
    "gradients_np = gradients_np / np.linalg.norm(gradients_np, axis = 1, keepdims=True) * 5\n",
    "times_np = all_times\n",
    "data_np = data_np_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata\n",
    "import scvelo as scv\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "\n",
    "# Assume your data is already defined\n",
    "# all_data: 50-dimensional data with shape (n_cells, 50)  \n",
    "# gradients: 50-dimensional vector field with shape (n_cells, 50)\n",
    "# pca: Trained PCA or UMAP model for generating X_umap\n",
    "# X_umap: Pre-computed UMAP embeddings with shape (n_cells, 2)\n",
    "\n",
    "# Create AnnData object\n",
    "adata = anndata.AnnData(X=all_data)\n",
    "\n",
    "# Set 'Ms' layer to avoid KeyError: 'Ms'\n",
    "adata.layers['Ms'] = all_data  # Use original 50D data as state matrix\n",
    "# Optional: normalize and log transform (based on data needs)\n",
    "# sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "# sc.pp.log1p(adata)\n",
    "# adata.layers['Ms'] = adata.X.copy()\n",
    "\n",
    "# Set velocity vectors\n",
    "adata.layers['velocity'] = gradients.detach().cpu().numpy()  # Store velocity vectors in layers\n",
    "print(adata.layers['velocity'].shape)  # Confirm shape\n",
    "\n",
    "# Set pre-computed UMAP embeddings\n",
    "if dim_reducer is not None:\n",
    "    X_umap = dim_reducer.transform(all_data)  # Assume pca is trained dim reduction model\n",
    "else:\n",
    "    X_umap = all_data[:,:2]\n",
    "adata.obsm['X_umap'] = X_umap\n",
    "print(adata.obsm['X_umap'].shape)  # Confirm UMAP embedding shape\n",
    "adata.obs['time'] = all_times\n",
    "if adata.layers['velocity'].shape[1] != 2:\n",
    "    # Compute neighbor graph (required for velocity graph)\n",
    "    sc.pp.neighbors(adata, n_neighbors=30, use_rep='X')  # Calculate neighbors based on high-dim data\n",
    "\n",
    "    # Compute velocity graph\n",
    "    scv.tl.velocity_graph(adata, vkey='velocity', n_jobs=16)  # Build velocity graph from high-dim velocity vectors\n",
    "\n",
    "    # Project velocities to UMAP space\n",
    "    scv.tl.velocity_embedding(adata, basis='umap', vkey='velocity')  # Project velocities to UMAP\n",
    "else:\n",
    "    adata.obsm['velocity_umap'] = adata.layers['velocity']\n",
    "\n",
    "# Plot\n",
    "adata.obs['time_categorical'] = pd.Categorical(adata.obs['time'])\n",
    "\n",
    "# Visualization settings\n",
    "scv.settings.set_figure_params('scvelo')  # Set scvelo plotting style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scv.pl.velocity_embedding_stream(\n",
    "    adata,\n",
    "    basis='umap',\n",
    "    color='time_categorical',\n",
    "    figsize=(7, 5),\n",
    "    density=3,\n",
    "    title='Score Stream Plot',\n",
    "    legend_loc='right',\n",
    "    palette='plasma',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "device = 'cpu'\n",
    "f_net.to(device)\n",
    "sf2m_score_model.to(device)\n",
    "# Assuming df, dim, device, f_net are defined\n",
    "all_times = df['samples'].values\n",
    "all_data = df[[f'x{i}' for i in range(1, dim + 1)]].values\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "t_tensor = torch.tensor(all_times, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "data_tensor = torch.tensor(all_data, dtype=torch.float32).to(device)\n",
    "\n",
    "# Calculate gradients\n",
    "with torch.no_grad():  # No need to track gradients to save memory\n",
    "    gradients = f_net.v_net(t_tensor, data_tensor)\n",
    "\n",
    "# Convert to NumPy array\n",
    "gradients_np = gradients.cpu().numpy()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Assume the following are defined:\n",
    "# - df: DataFrame with columns 'samples', 'x1', 'x2', ..., 'xn'\n",
    "# - dim: Number of dimensions (n)\n",
    "# - device: torch.device (e.g., 'cpu' or 'cpu')\n",
    "# - sf2m_score_model: Your model function that takes t_tensor and data_tensor\n",
    "\n",
    "# Step 1: Extract all time points and data points\n",
    "all_times = df['samples'].values\n",
    "all_data = df[[f'x{i}' for i in range(1, dim + 1)]].values\n",
    "\n",
    "#t_value = 4.0  \n",
    "#t_tensor = torch.tensor([t_value] * all_data.shape[0]).unsqueeze(1).float().to(device)\n",
    "t_tensor = torch.tensor(all_times, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "# Step 2: Convert to PyTorch tensors\n",
    "data_tensor = torch.tensor(all_data, dtype=torch.float32).to(device)\n",
    "data_tensor.requires_grad_(True)  # Enable gradient tracking\n",
    "#t_tensor = torch.tensor(all_times, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "# Step 3: Compute log density values and gradients\n",
    "log_density_values = sf2m_score_model(t_tensor, data_tensor)\n",
    "log_density_values.backward(torch.ones_like(log_density_values))\n",
    "gradients_score = data_tensor.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall=gradients_np+gradients_score.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata\n",
    "import scvelo as scv\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "all_data = df[[f'x{i}' for i in range(1, dim + 1)]].values\n",
    "\n",
    "# Assume data is defined: all_data, gradients, X_umap, all_times, cell_type\n",
    "adata = anndata.AnnData(X=all_data)\n",
    "adata.layers['Ms'] = all_data\n",
    "adata.layers['velocity'] = overall\n",
    "adata.obsm['X_umap'] = data_np\n",
    "adata.obs['time'] = all_times\n",
    "velocity_norm = adata.layers['velocity'] \n",
    "# Verify shapes\n",
    "print(\"Velocity shape:\", adata.layers['velocity'].shape)\n",
    "print(\"UMAP shape:\", adata.obsm['X_umap'].shape)\n",
    "\n",
    "# Compute neighborhood graph\n",
    "#sc.pp.neighbors(adata, n_neighbors=30, use_rep='X')\n",
    "\n",
    "# Compute velocity graph\n",
    "#scv.tl.velocity_graph(adata, vkey='velocity', n_jobs=1)\n",
    "#\n",
    "# Handle velocity projection\n",
    "if adata.layers['velocity'].shape[1] != 2:\n",
    "    sc.pp.neighbors(adata, n_neighbors=30, use_rep='X')\n",
    "    scv.tl.velocity_graph(adata, vkey='velocity', n_jobs=16)\n",
    "    scv.tl.velocity_embedding(adata, basis='umap', vkey='velocity')\n",
    "else:\n",
    "    adata.obsm['velocity_umap'] = adata.layers['velocity']\n",
    "\n",
    "# Plot\n",
    "\n",
    "adata.obs['time_categorical'] = pd.Categorical(adata.obs['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scv.settings.set_figure_params('scvelo')\n",
    "scv.pl.velocity_embedding_stream(\n",
    "    adata,\n",
    "    basis='umap',\n",
    "    color='time_categorical',\n",
    "    figsize=(7, 5),\n",
    "    density=1,\n",
    "    title='All velocity Stream Plot',\n",
    "    legend_loc='right',\n",
    "    palette='viridis',\n",
    "    save=exp_dir+'/all_velocity_stream_plot.svg'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepRUOT.utils import euler_sdeint\n",
    "import random\n",
    "n_times = all_times.max() + 1\n",
    "data=torch.tensor(df[df['samples']==0].values,dtype=torch.float32).requires_grad_()\n",
    "data_t0 = data[:, 1:].to(device).requires_grad_()\n",
    "print(data_t0.shape)\n",
    "x0=data_t0.to(device)\n",
    "\n",
    "class SDE(torch.nn.Module):\n",
    "    noise_type = \"diagonal\"\n",
    "    sde_type = \"ito\"\n",
    "\n",
    "    def __init__(self, ode_drift, g, score, input_size=(3, 32, 32), sigma=1.0):\n",
    "        super().__init__()\n",
    "        self.drift = ode_drift\n",
    "        self.score = score\n",
    "        self.input_size = input_size\n",
    "        self.sigma = sigma\n",
    "        self.g_net = g\n",
    "\n",
    "    # Drift\n",
    "    def f(self, t, y):\n",
    "        z, lnw = y\n",
    "        drift=self.drift(t, z)\n",
    "        dlnw = self.g_net(t, z)\n",
    "        num = z.shape[0]\n",
    "        t = t.expand(num, 1)  # Keep gradient information of t and expand its shape\n",
    "        return (drift+self.score.compute_gradient(t, z), dlnw)\n",
    "\n",
    "    # Diffusion\n",
    "    def g(self, t, y):\n",
    "        return torch.ones_like(y)*self.sigma\n",
    "    \n",
    "x0_subset = x0.to(device)\n",
    "\n",
    "x0_subset = x0_subset.to(device)\n",
    "lnw0 = torch.log(torch.ones(x0_subset.shape[0], 1) / x0_subset.shape[0]).to(device)\n",
    "initial_state = (x0_subset, lnw0)\n",
    "\n",
    "# Define SDE object\n",
    "sde = SDE(f_net.v_net, \n",
    "          f_net.g_net, \n",
    "          sf2m_score_model, \n",
    "          input_size=(2,), \n",
    "          sigma=config['score_train']['sigma'])\n",
    "\n",
    "# Define time points, assuming total 200 integration steps\n",
    "ts = torch.linspace(0, n_times - 1, 100, device=device)\n",
    "\n",
    "# Manual SDE integration\n",
    "sde_traj, traj_lnw = euler_sdeint(sde, initial_state, dt=0.1, ts=ts)\n",
    "# Transfer to CPU if needed:\n",
    "sde_traj, traj_lnw = sde_traj.cpu(), traj_lnw.cpu()\n",
    "\n",
    "\n",
    "sample_number = 100  # For example, sample 10\n",
    "sample_indices = random.sample(range(sde_traj.size(1)), sample_number)\n",
    "sampled_sde_trajec = sde_traj[:, sample_indices, :]\n",
    "sampled_sde_trajec.shape\n",
    "sampled_sde_trajec = sampled_sde_trajec.tolist()\n",
    "sampled_sde_trajec = np.array(sampled_sde_trajec, dtype=object)\n",
    "np.save(exp_dir+'/sde_trajec.npy', sampled_sde_trajec)\n",
    "\n",
    "\n",
    "ts_points = torch.arange(n_times, device=device)\n",
    "print(ts_points)\n",
    "\n",
    "sde_point, traj_lnw = euler_sdeint(sde, initial_state, dt=0.1, ts=ts_points)\n",
    "print(sde_point.shape)\n",
    "print(traj_lnw.shape)\n",
    "weight = torch.exp(traj_lnw)\n",
    "weight_normed = weight/weight.sum(dim = 1, keepdim = True)\n",
    "\n",
    "sde_point_np = sde_point.detach().cpu().numpy()\n",
    "sde_point_list = sde_point_np.tolist()\n",
    "sde_point_array = np.array(sde_point_list, dtype=object)\n",
    "np.save(exp_dir+'/sde_point.npy', sde_point_array)\n",
    "np.save(exp_dir+'/sde_weight.npy', weight_normed.detach().cpu().numpy())\n",
    "\n",
    "from DeepRUOT.plots import new_plot_comparisions2\n",
    "sde_point = np.load(exp_dir+'/sde_point.npy', allow_pickle=True)\n",
    "sde_point.shape\n",
    "\n",
    "sde_trajec = np.load(exp_dir+'/sde_trajec.npy', allow_pickle=True)\n",
    "samples = df.iloc[:, 0]  # Get samples column\n",
    "features = df.iloc[:, 1:]  # Get feature columns\n",
    "\n",
    "# Transform data based on whether dim_reducer is provided\n",
    "if dim_reducer is not None:\n",
    "    # Transform original data\n",
    "    reduced_features = dim_reducer.transform(features)\n",
    "    \n",
    "    # Transform generated points\n",
    "    generated_flattened = sde_point.reshape(-1, features.shape[1])\n",
    "    generated_reduced = dim_reducer.transform(generated_flattened)\n",
    "    generated_reduced = generated_reduced.reshape(sde_point.shape[0], -1, 2)\n",
    "    \n",
    "    # Transform trajectories \n",
    "    trajectories_flattened = sde_trajec.reshape(-1, features.shape[1])\n",
    "    trajectories_reduced = dim_reducer.transform(trajectories_flattened)\n",
    "    trajectories_reduced = trajectories_reduced.reshape(sde_trajec.shape[0], -1, 2)\n",
    "else:\n",
    "    # Use first two dimensions\n",
    "    reduced_features = features.iloc[:,:2].values\n",
    "    generated_reduced = sde_point[:,:,:2] \n",
    "    trajectories_reduced = sde_trajec[:,:,:2]\n",
    "\n",
    "# Create dataframe with reduced dimensions\n",
    "new_df = pd.DataFrame(reduced_features, columns=['x1', 'x2'])\n",
    "new_df['samples'] = samples\n",
    "\n",
    "new_plot_comparisions2(\n",
    "    new_df, generated_reduced, trajectories_reduced,\n",
    "    palette='viridis', df_time_key='samples', \n",
    "    save=True, path=exp_dir, file='sde_trajectories.pdf',\n",
    "    x='x1', y='x2', z='x3', is_3d=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
